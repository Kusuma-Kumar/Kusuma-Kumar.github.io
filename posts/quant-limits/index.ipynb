{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Limits of the Quantitative Approach\"\n",
    "author: Kusuma Kumar\n",
    "date: '2025-03-19'\n",
    "image: \"image.png\"\n",
    "description: \"Exploring the limitations of a purely quantitative approach to understanding algorithmic bias and discrimination.\"\n",
    "format: html\n",
    "bibliography: refs.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Quantitative methods are widely used to assess bias and discrimination in machine learning models, yet their effectiveness remains a topic of debate. While these methods—such as statistical fairness criteria, audits, and hypothesis testing—provide objective ways to detect disparities, they also have limitations. @narayanan2022limits critiques these approaches, arguing that they often reinforce systemic bias rather than mitigate it. This essay engages with Narayanan’s claims by analyzing both the benefits and drawbacks of quantitative fairness assessments. Using examples from facial recognition technology and Facebook’s housing ad delivery algorithms, it explores the trade-offs between mathematical fairness notions and broader ethical concerns. Drawing from @barocasFairnessMachineLearning2023 and @D20201, this analysis highlights how fairness metrics alone cannot resolve deep-seated inequalities. Instead, a combined approach—integrating statistical methods with qualitative and policy-driven interventions—is essential for achieving meaningful algorithmic fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narayanan’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍position\n",
    " \n",
    "Narayanan identifies seven key limitations of these methods, which, despite good intentions, can justify racism and perpetuate inaction. He highlights the null hypothesis issue—the assumption that choices reflect the perspective of privileged individuals who do not experience daily discrimination. Narayanan argues that data are not neutral but inherently political. Collected with specific goals, data can obscure or emphasize aspects of reality. Thus, scientific and quantitative methods cannot resolve fundamental conflicts over values.\n",
    "\n",
    "He cautions against viewing quantitative methods as objective alternatives to democratic debates on societal values. Instead, he advocates for greater emphasis on descriptive work—collecting, curating, and carefully describing data without relying solely on complex statistical models. Narayanan calls for a more critical approach to data analysis, urging researchers to interrogate datasets, understand their context, and recognize embedded assumptions.\n",
    "\n",
    "While he does not reject quantitative methods, he advocates for fundamentally shifting their role in studying discrimination. He calls for actions that restore context and meaning to numerical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍quantitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍methods\n",
    "\n",
    "The article “Evaluating Impact of Race in Facial Recognition across Machine Learning and Deep Learning Algorithms” demonstrates how quantitative methods provide crucial insights into racial bias in facial recognition technology. By leveraging objective metrics, these methods enable a systematic and data-driven approach to identifying disparities across different racial groups.\n",
    "\n",
    "Quantitative analysis offers concrete evidence of bias by measuring key performance indicators such as accuracy, false positive rates, and false negative rates across diverse datasets (@coe2021evaluating). For instance, the study reveals that facial recognition models trained on racially imbalanced datasets (primarily skewed toward White individuals) exhibit lower true positive rates and higher false negative rates for Black individuals, particularly with Decision Trees and Logistic Regression. This imbalance indicates a clear violation of error rate parity, a fairness criterion which holds that different groups should have similar error rates (@barocasFairnessMachineLearning2023). When one group experiences disproportionately more misclassifications, the system enacts a systematic disadvantage, even if sensitive attributes are not explicitly included in the model.\n",
    "\n",
    "While confusion matrices help visualize these disparities, fairness metrics like equal opportunity (ensuring equal true positive rates across groups) and predictive parity (ensuring that predictive values are consistent across groups) provide a stronger evaluative framework. The case study’s findings align with concerns raised in Chapter 4’s “middle view”, which recognizes that even if a model performs well in aggregate, it may still violate principles of equal opportunity if disadvantaged subgroups face lower accuracy. In this sense, the quantitative methods reveal how a seemingly neutral algorithm can perpetuate inequity—a concern emphasized by the broad view as well, which situates algorithmic fairness within larger historical and social contexts of oppression.\n",
    "\n",
    "In this case, quantitative methods don't merely diagnose bias; they expose how model design, training data, and evaluation practices can combine to reproduce structural inequalities. As @narayanan2022limits argues, although statistical metrics cannot fully capture fairness, they are essential tools for understanding when and how algorithms might violate moral expectations of equal treatment. These findings also support arguments from data feminism (@D20201), advocating for justice-oriented data practices that prioritize the experiences of those most harmed by technical systems.\n",
    "\n",
    "By grounding fairness evaluations in rigorous, quantifiable terms like error rate parity or calibration, developers and policymakers are better equipped to not only recognize harm but to mitigate it. These quantitative tools empower us to ask better questions about who who gets left behind while developing these algorithmic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawbacks ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍quantitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍methods\n",
    "\n",
    "Quantitative methods are frequently used to assess discrimination in algorithmic systems, but they face significant limitations, particularly in contexts where powerful entities control access to data and where machine learning models optimize outcomes in ways that reinforce systemic bias. The National Fair Housing Alliance v. Facebook, Inc. case illustrates these challenges, showing that quantitative fairness assessments can be insufficient or misleading when applied to complex, opaque digital advertising platforms.\n",
    "\n",
    "Reliance on Corporate-Controlled Data\n",
    "One major limitation of quantitative methods in discrimination audits is their reliance on data provided by the very companies under scrutiny. As @narayanan2022limits argues, researchers and policymakers often lack independent access to platform data, making it difficult to verify whether algorithmic processes are truly unbiased. This is particularly evident in cases like National Fair Housing Alliance v. Facebook, Inc., where legal challenges relied on Facebook’s own datasets to evaluate discrimination in housing ad delivery. Because companies like Facebook have a vested interest in minimizing evidence of discrimination, they control what data is disclosed, how it is structured, and how it is analyzed, often in ways that obscure the full extent of bias.\n",
    "\n",
    "Even when researchers gain access to datasets, biases in data collection and reporting can undermine fairness assessments. @10.1145/3419394.3423641 demonstrates that digital advertising platforms allow advertisers to combine seemingly neutral targeting attributes in ways that systematically disadvantage protected groups, even when explicit targeting by race, gender, or age is restricted. These subtle forms of bias are difficult to detect using conventional statistical tests, leading to an illusion of fairness when discrimination is still occurring.\n",
    "\n",
    "Algorithmic Optimization and Hidden Bias\n",
    "Beyond issues of data control, quantitative methods often fail to account for hidden biases introduced by algorithmic optimization. As @humber2020fairhousing explains, even when explicit targeting mechanisms are removed, Facebook’s ad delivery system independently learns to distribute ads in ways that reinforce historical inequalities. For example, research has shown that housing ads featuring imagery associated with white communities are disproportionately shown to white users, while ads with content stereotypically linked to Black communities are more frequently delivered to Black users. These disparities arise not from advertiser intent but from Facebook’s internal algorithmic processes, which optimize for engagement in ways that mirror and perpetuate existing social divisions.\n",
    "\n",
    "This presents a key limitation of fairness audits: most quantitative assessments focus on explicit discrimination in inputs (such as targeting criteria) rather than biased outcomes in ad delivery. As @narayanan2022limits warns, this narrow focus allows platforms to claim compliance with anti-discrimination laws while continuing to produce biased results through automated decision-making. The inability of quantitative models to detect these second-order effects underscores the need for alternative approaches that combine statistical techniques with qualitative investigations, policy interventions, and platform accountability measures.\n",
    "\n",
    "Structural Bias and the Limits of Statistical Fairness\n",
    "Another critical drawback is that quantitative fairness metrics often fail to capture structural inequalities embedded in machine learning models. For example, fairness metrics like demographic parity and equalized odds focus on statistical balance across groups but do not account for historical and systemic disadvantages. This is particularly problematic in areas like housing and employment, where past discrimination has created lasting disparities that bias training data and shape algorithmic outputs in ways that simple fairness metrics cannot correct.\n",
    "\n",
    "Furthermore, @humber2020fairhousing highlights that current legal frameworks struggle to hold platforms accountable because quantitative evidence alone is often insufficient to prove intent or causality. Even if researchers identify disparities in ad delivery, platforms like Facebook can argue that these results are incidental rather than intentional, making it difficult to enforce fair housing laws in digital spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections to Chapter 3, 4\n",
    "\n",
    "Quantitative fairness notions, such as demographic parity and equalized odds, provide structured ways to measure and mitigate bias but also have limitations. As @barocasFairnessMachineLearning2023 explains, fairness criteria inherently involve trade-offs, meaning no single metric fully captures ethical concerns.\n",
    "\n",
    "In facial recognition bias, statistical measures like false positive rates and false negative rates align with equalized odds, which requires similar error rates across groups (@barocasFairnessMachineLearning2023). Research by Coe and Atay (2021) demonstrates that models trained on imbalanced datasets disproportionately misclassify Black individuals, highlighting systematic disadvantage—a central issue in algorithmic fairness (@barocasFairnessMachineLearning2023). While statistical fairness measures diagnose disparities, they do not dictate whether these disparities are morally or legally acceptable.\n",
    "\n",
    "In the National Fair Housing Alliance v. Facebook, Inc. case, demographic parity was a key concern, as Facebook’s ad delivery system disproportionately limited housing opportunities for marginalized communities. While demographic parity suggests that outcomes should be independent of protected attributes. @barocasFairnessMachineLearning2023 (Ch. 3) notes that enforcing strict parity can conflict with calibration, which ensures accurate predictions. Even when explicit targeting is restricted, ad delivery optimizations perpetuate historical inequalities (Humber and Matthews, 2020). This exemplifies how satisfying one fairness metric can still lead to inequitable outcomes—a core issue explored in Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix of Domination\n",
    "\n",
    "Facebook's advertising platform operates within multiple domains:\n",
    "\n",
    "* Hegemonic domain: Facebook's ad targeting options, even if seemingly neutral on their face after restrictions were put in place, could be used in ways that reinforced existing discriminatory ideas about who belongs in certain neighborhoods. Advertisers might have subtly employed combinations of targeting features or even the ad content itself to exclude certain demographic groups from seeing housing opportunities. This aligns with the hegemonic function of \"circulating ideas about who is entitled to exercise power and who is not\"(@D20201).\n",
    "\n",
    "* Interpersonal Domain: If individuals were systematically less likely to see housing ads due to Facebook's algorithms, they would have missed out on crucial information about available homes. This directly affects their ability to find suitable and affordable housing, a fundamental aspect of their lives. Experiencing the exclusion from housing information, even if the exact cause was not immediately apparent, could contribute to feelings of being systematically disadvantaged. This can reinforce existing societal inequalities in housing access on a personal level, impacting individuals' sense of belonging and opportunity by conveying that \"systems of power are not on [their] side\" (@D20201)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My views\n",
    "\n",
    "Narayanan's speech holds significant truth and carries weight in the decisions we make in this industry. I agree with his concerns but reject the notion that quantitative methods do “more harm than good” (@narayanan2022limits). The harm lies not in the methods themselves but in how they are applied. When used critically and in conjunction with broader ethical considerations, they serve as powerful tools for identifying and mitigating algorithmic discrimination rather than merely justifying existing inequalities.\n",
    "\n",
    "I align with the middle view of fairness, which emphasizes the importance of addressing historical disadvantages faced by individuals. I believe that compensating for these inequities is crucial in the development of fairer systems, although it is important to recognize that algorithmic bias is inherently embedded in the data used to train these models, and thus, cannot be fully eradicated.\n",
    "\n",
    "Under the middle view, it is the responsibility of the algorithm designer to determine how best to account for diversity and individual variation in their models. This perspective acknowledges both the need for fairness in individual treatment and the challenges inherent in working with biased data. However, it is equally important for those who deploy and utilize these algorithms to adopt a broad view of fairness. The broad view shifts the focus from individual-level adjustments to systemic-level interventions, aiming to address and mitigate injustices that arise at the societal level. This dual approach—balancing the individual adjustments of the middle view with the systemic focus of the broad view—ensures a more comprehensive strategy for combating discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "@narayanan2022limits raises fundamental concerns about the ability of quantitative fairness methods to address discrimination. While measures such as equalized odds and demographic parity provide structured ways to assess fairness, they fail to capture historical and structural inequalities embedded in data. As discussed in @barocasFairnessMachineLearning2023, fairness metrics inherently involve trade-offs, meaning no single metric can fully address ethical concerns.\n",
    "\n",
    "The case studies examined in this essay illustrate these challenges. In facial recognition systems, quantitative fairness audits have been useful in exposing racial disparities (@coe2021evaluating), yet they do not eliminate the root causes of bias in training data. Similarly, in the National Fair Housing Alliance v. Facebook, Inc. case, fairness audits identified disparities in ad delivery, but they could not fully explain or prevent algorithmic discrimination. Facebook’s ad targeting system perpetuated segregation even when explicit demographic data was removed, demonstrating that bias often emerges from optimization processes rather than direct human intent (@humber2020fairhousing).\n",
    "\n",
    "Despite these limitations, abandoning quantitative methods is not a viable solution. As seen in fairness audits of facial recognition models, these methods remain essential for diagnosing disparities and prompting regulatory scrutiny. However, as @D20201 argues, fairness cannot be achieved through mathematical models alone—it requires a broader, justice-oriented approach that incorporates qualitative insights, historical context, and systemic interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
