[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/automated-decision-systems/index.html",
    "href": "posts/automated-decision-systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This blog post explores the design, impact, and performance of automated decision-making systems in the context of credit risk evaluation. Using a real-world dataset of loan applicants, we perform exploratory data analysis to understand demographic patterns and loan behavior across age, income, and homeownership groups. We apply logistic regression to predict loan default risk and evaluate model performance using classification metrics and profit-based threshold optimization. Our findings highlight both the predictive strengths and limitations of simple models like logistic regression, particularly in handling class imbalance. We further examine the trade-offs between loan approval and profit maximization, offering insights into how automated decision systems can influence financial access and institutional outcomes.\nAccess the training data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.dropna()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\nN\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n22907 rows × 12 columns"
  },
  {
    "objectID": "posts/automated-decision-systems/index.html#figure-1",
    "href": "posts/automated-decision-systems/index.html#figure-1",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Figure 1",
    "text": "Figure 1\nThe code below performs a data analysis and visualization task focused on loan intent distribution across various age groups and types of homeownership. We first filters the data by selecting only relevant columns (person_age, person_home_ownership, and loan_intent) from the df_train DataFrame, while excludeing any records where the person_home_ownership is labeled as “OTHER”. Next, we creates a new column, age_group, by dividing the person_age into predefined age bins (18-24, 25-34, etc.) using the pd.cut() function, which categorizes the individuals into different age ranges. The code then groups the data by age_group, person_home_ownership, and loan_intent, calculating the count of applicants for each combination. Finally, it creates a series of bar plots with the x-axis representing age groups, the y-axis representing the count of applicants, and the hue denoting the loan intent. The plots are further divided by the type of homeownership (e.g., “OWN” or “RENT”), allowing for a comparative visualization across different ownership types.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_1 = df_train[[\"person_age\", \"person_home_ownership\", \"loan_intent\"]]\ndf_1 = df_1[df_1[\"person_home_ownership\"] != \"OTHER\"]\nbins = [18, 25, 35, 45, 55, 65, 75, 100]\nlabels = ['18–24', '25–34', '35–44', '45–54', '55–64', '65–74', '75+']\ndf_1['age_group'] = pd.cut(df_1['person_age'], bins=bins, labels=labels, right=False)\n\ngrouped = df_1.groupby(\n    ['age_group', 'person_home_ownership', 'loan_intent']\n    ).size().reset_index(name='count')\n\nfig_1 = sns.catplot(\n    data=grouped,\n    x=\"age_group\",\n    y=\"count\",\n    hue=\"loan_intent\",\n    col=\"person_home_ownership\",\n    kind=\"bar\"\n)\n\nfig_1.set_axis_labels(\"Age Group (Years)\", \"Number of Applicants\")\nfig_1.set_titles(\"Home Ownership: {col_name}\")\nfig_1.fig.suptitle('Loan Intent by Age Group and Type of Home Ownership', fontsize=16)\nfig_1.fig.subplots_adjust(top=0.85)\n\nplt.show()\n\n/var/folders/97/pjhs17cj6y5b0m1x31sjmszh0000gn/T/ipykernel_82493/1813685786.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped = df_1.groupby(\n\n\n\n\n\n\n\n\n\nThe output of the visualizations presents the distribution of loan intent by age group and homeownership type. From the plots, we can observe clear patterns regarding loan intent preferences within each age group. For instance, there are fewever number of loan applicantions from those who do own a home and across all age groups there is a higher tendency for these to be for venture. Younger individuals in the 18–24 age group tend to show a higher intent for education regardless of home ownership status but we do observe that those who do own a home lean slighly towar venture loans. Those in the 25–34 age group show more balanced intentions between all types of loans when they are paying their mortgage or owning with slightly higher loand applications in venture if they do own. For those &gt; 25 years old who are renting, more applications tend to be for medical bills with the least for home improvement."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html#figure-2",
    "href": "posts/automated-decision-systems/index.html#figure-2",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Figure 2",
    "text": "Figure 2\nThe code begins by displaying the maximum and minimum values of the loan_int_rate column, which is used to understand the range of loan interest rates in the dataset. The loan_int_rate values are then categorized into predefined bins, ranging from 5% to 23.9%. Each bin represents a specific range of interest rates, such as 5-5.9%, 6-6.9%, etc. The pd.cut() function is used to create these bins and label them with meaningful ranges. Following this, the code computes aggregated statistics for each bin, specifically the mean and median values of two other variables: cb_person_cred_hist_length (credit history length) and loan_percent_income (income percentage). These statistics provide insights into how credit history and income change as the loan interest rate increases, which could reveal underlying patterns or correlations in the dataset.\n\nprint(\"Max Value:\", df_train['loan_int_rate'].max())\nprint(\"Min Value:\", df_train['loan_int_rate'].min())\n\nbins = list(range(5, 25))\nlabels = [f\"{i}-{i+0.9}\" for i in bins[:-1]] \ndf_train['int_rate'] = pd.cut(df_train['loan_int_rate'], bins=bins, labels=labels, right=False)\n\ndf_train.groupby('int_rate').agg(\n    mean_cred_hist_length=('cb_person_cred_hist_length', 'mean'),\n    median_cred_hist_length=('cb_person_cred_hist_length', 'median'),\n    mean_percent_income=('loan_percent_income', 'mean'),\n    median_income=('loan_percent_income', 'median')\n)\n\nMax Value: 23.22\nMin Value: 5.42\n\n\n/var/folders/97/pjhs17cj6y5b0m1x31sjmszh0000gn/T/ipykernel_82493/4186742674.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df_train.groupby('int_rate').agg(\n\n\n\n\n\n\n\n\n\nmean_cred_hist_length\nmedian_cred_hist_length\nmean_percent_income\nmedian_income\n\n\nint_rate\n\n\n\n\n\n\n\n\n5-5.9\n5.519231\n4.0\n0.134936\n0.110\n\n\n6-6.9\n5.740470\n4.0\n0.155953\n0.130\n\n\n7-7.9\n5.767098\n4.0\n0.157283\n0.130\n\n\n8-8.9\n5.792246\n4.0\n0.163628\n0.140\n\n\n9-9.9\n5.861830\n4.0\n0.165356\n0.140\n\n\n10-10.9\n5.669600\n4.0\n0.176271\n0.150\n\n\n11-11.9\n5.766530\n4.0\n0.170990\n0.150\n\n\n12-12.9\n5.809459\n4.0\n0.178317\n0.160\n\n\n13-13.9\n5.862788\n4.0\n0.173296\n0.160\n\n\n14-14.9\n5.858747\n4.0\n0.175952\n0.160\n\n\n15-15.9\n5.800321\n4.0\n0.181844\n0.170\n\n\n16-16.9\n5.877381\n4.0\n0.194369\n0.180\n\n\n17-17.9\n5.697329\n4.0\n0.201929\n0.180\n\n\n18-18.9\n6.479769\n6.0\n0.214277\n0.210\n\n\n19-19.9\n6.427273\n4.0\n0.210545\n0.205\n\n\n20-20.9\n6.121951\n5.0\n0.210000\n0.200\n\n\n21-21.9\n4.909091\n4.0\n0.165455\n0.140\n\n\n22-22.9\n6.666667\n8.0\n0.116667\n0.110\n\n\n23-23.9\n12.000000\n12.0\n0.270000\n0.270\n\n\n\n\n\n\n\nLooking at the output table, we observe some distinct trends. The mean credit history length remains quite consistent across most of the interest rate bins, hovering around 5.7 years. However, the median credit history length stays constant at 4 years for the majority of the bins, with a noticeable exception at the higher interest rate bins, such as 18-18.9% where it increases to 6 years. For the loan_percent_income variable, we see that as the interest rate increases, the average percentage of income dedicated to loan payments tends to rise slightly, suggesting that individuals with higher loan interest rates may be allocating a greater portion of their income to servicing the loan. The median income percentage also reflects this trend, showing higher values for higher interest rate bins. In particular, the highest interest rate bin (23-23.9%) stands out with a sharp rise in both average credit history length (12 years) and income percentage (27%), indicating that this group may consist of a specific subset of borrowers with longer credit histories and higher financial obligations."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html#figure-3",
    "href": "posts/automated-decision-systems/index.html#figure-3",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Figure 3",
    "text": "Figure 3\nThe code begins by creating a subset of the original dataset (df_3) that includes the variables ‘person_income’, ‘loan_amnt’, ‘person_emp_length’, and ‘cb_person_default_on_file’. The next step categorizes ‘person_income’ into predefined income groups, ranging from 0 to 150k, using the pd.cut() function. This allows the income distribution to be analyzed in segments. The code also calculates the loan amount thresholds (the 25th and 75th percentiles) to classify borrowers into ‘Low’, ‘Mid’, or ‘High’ credit access categories based on their loan amount using a custom function (categorize_credit_access). Finally, a sns.catplot is used to visualize the distribution of income levels and credit access, with data split by whether the borrower has a default on file (‘cb_person_default_on_file’).\n\ndf_3 = df_train[['person_income', 'loan_amnt', 'person_emp_length', 'cb_person_default_on_file']].copy()\n\nbins = [0, 30000, 60000, 90000, 120000, 150000, float('inf')]\nlabels = ['0-30k', '30k-60k', '60k-90k', '90k-120k', '120k-150k', '150k+']\n\ndf_3.loc[:, 'income_level'] = pd.cut(df_3['person_income'], bins=bins, labels=labels, right=False)\n\nlow_threshold = df_3['loan_amnt'].quantile(0.25)\nhigh_threshold = df_3['loan_amnt'].quantile(0.75)\n\ndef categorize_credit_access(amount):\n    if amount &lt;= low_threshold:\n        return 'Low'\n    elif amount &lt;= high_threshold:\n        return 'Mid'\n    else:\n        return 'High'\n\ndf_3.loc[:, 'credit_access'] = df_3['loan_amnt'].apply(categorize_credit_access)\n\nfig_3 = sns.catplot(\n    x='income_level',\n    hue='credit_access',\n    col='cb_person_default_on_file',\n    data=df_3,\n    kind='count'\n)\n\nfig_3.set_titles('Default on File: {col_name}')\nfig_3.set_axis_labels('Income Level', 'Number of Borrowers')\nfig_3.set_xticklabels(rotation=45)\nfig_3._legend.set_title('Credit Access')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe output from Figure 3 provides a detailed breakdown of credit access across different income levels and whether borrowers have a default on file. From the plot, it is apparent that higher income segments, particularly those in the ‘120k-150k’ and ‘150k+’ income categories, tend to have greater access to higher credit lines (‘High’ credit access) regardless of wether they have defaulted. We also observe that there are fewer applicants regardless of income level that have defaulted on a previous loan. From those applicant who have not defaulted, if they are low-mid income level(30k - 90k) have higher access to mid loan amount than higher loan amounts. As expected we observe that low income applicatnts have little - no access to large loans with some access to medium loan amounts but most access to smaller loans."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nThis blog post explores the process of building a machine learning classifier using the Palmer Penguins dataset. The analysis focuses on the identification of key features and the construction of a Random Forest model to classify penguin species based on various measurements. Through data preparation, feature engineering, and model training, we assess how well different combinations of features influence the classifier’s performance. The results reveal that a perfect accuracy of 100% can be achieved, with decision region plots visualizing the classifier’s ability to distinguish between species and sexes. The findings highlight the significant impact of feature selection and the effectiveness of Random Forest classifiers in this scenario.\nAcess the training data and test data\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\n\nData Preparation\nYou ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍will ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍need to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍prepare ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ ‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍qualitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Categorical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍feature ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍like ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Sex ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Island ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍should ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍converted ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍so-called ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍“one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍encoded” ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍0-1 ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍using ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pd.get_dummies ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍column ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Species ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍should ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍coded ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍differently, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍using ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍LabelEncoder. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍following ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍handles ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍this ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍work ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_test, y_test = prepare_data(test)\n\n\n\nSummary table\nThe following code processes the “Species” column of a train DataFrame by splitting any compound species names and extracting only the first word (e.g., “Adelie” from “Adelie Penguin”). It then creates a new DataFrame df that includes only the relevant columns: “Body Mass (g)”, “Island”, and “Species”. Next, it groups the data by both “Species” and “Island” to compute three statistics for the “Body Mass (g)” column: the mean, median, and count of observations. This aggregation provides a summary table that reveals the average body mass (mean), the middle value (median), and the number of penguin observations (count) for each species on each island, allowing for a clear comparison of body mass distributions across different species and locations.\nBased on the summary statistics provided, it’s clear that only Adelie penguins are found on all three islands, while Chinstrap and Gentoo penguins are restricted to only specific islands - Dream and Biscoe respectively. Adelie species data reveals that living on different islands has a negligible impact on body mass as mean and median body mass values for Adelie penguins are almost identical across the three islands. In contrast, Chinstrap penguins on Dream Island have a mean body mass of around 3743g, and Gentoo penguins on Biscoe Island have a much higher mean of 5039g, reflecting the species’ differing size characteristics.\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ndf = train[[\"Body Mass (g)\", \"Island\", \"Species\"]]\n\ndf.groupby(['Species' ,'Island' ])['Body Mass (g)'].agg(\n    Mean='mean',\n    Median='median',\n    Count='count'\n)\n\n\n\n\n\n\n\n\n\nMean\nMedian\nCount\n\n\nSpecies\nIsland\n\n\n\n\n\n\n\nAdelie\nBiscoe\n3711.363636\n3750.0\n33\n\n\nDream\n3728.888889\n3700.0\n45\n\n\nTorgersen\n3712.804878\n3700.0\n41\n\n\nChinstrap\nDream\n3743.421053\n3700.0\n57\n\n\nGentoo\nBiscoe\n5039.948454\n5000.0\n97\n\n\n\n\n\n\n\n\n\nFigure 1\nThe code below visualizes the relationship between Flipper Length and Culmen Length across the different penguin species. The scatterplot clearly reveals three distinct clusters corresponding to the three species: Adelie, Gentoo, and Chinstrap. The Adelie species primarily lies in the lower-left corner of the plot, while Gentoo penguins are clustered in the upper-left, and Chinstrap penguins are positioned towards the lower-right. Despite the clear separation, there are a few data points that overlap between species. Notably, there is some mixing of Adelie and Chinstrap penguins in the middle of the plot, which suggests potential similarities in their flipper and culmen lengths, making them harder to distinguish in certain cases.\n\nimport seaborn as sns\n\ndf = train[[\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Species\"]]\nfigure1 = sns.scatterplot(df, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\n\n\n\nFigure 2\nI wanted to explore the relationship between Sex, Culmen Length, and Delta 15 N to examine potential differences between male and female penguins within the Chinstrap species. Specifically, I was curious to see if there were any notable patterns or trends in these two measurements that could vary by sex. By plotting Culmen Length (mm) against Delta 15 N, we can get a sense of how these two variables correlate and whether there are any distinguishing features between males and females of the Chinstrap species.\nFrom the scatter plot, we observe that both males and females within the Chinstrap species show a similar spread across Delta 15 N, but with the females’ values slightly lower. The notable difference, however, is that females seem to have shorter Culmen Lengths compared to males, although there is slight overlap around 50–51 mm. There is also one noticeable outlier: a female Chinstrap with the highest Culmen Length of 58 mm.\n\ndf = train[[\"Species\", \"Sex\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nfiltered_df = df[df['Species'] == \"Chinstrap\"]\nfigure2 = sns.scatterplot(filtered_df, x = \"Culmen Length (mm)\", y = \"Delta 15 N (o/oo)\", hue = \"Sex\", style = \"Sex\")\n\n\n\n\n\n\n\n\n\n\nUsing Findings from Figure to Create Model\nThe scatterplots showing the relationship between flipper length and culmen length revealed distinct clusters for each penguin species, which highlighted the importance of these features for classification. I used this information to carefully select and pair quantitative features that showed strong separation between species, such as flipper and culmen lengths, to train the Random Forest classifier. Additionally, the scatterplot of Culmen Length vs. Delta 15 N for Chinstrap penguins provided further insights into the potential influence of sex on species classification, prompting me to include the ‘Sex’ feature in the model. The decision region plots helped me visualize how different feature combinations influenced the classifier’s ability to distinguish between species. This visualization reinforced the importance of feature selection and showed me how decision boundaries could be altered with different combinations of features.\n\n\nPlotting Decision Regions\nNow ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍going ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍panel ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍decision ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍classifier. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍You ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍can ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍use ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot_regions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defined ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍below ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regions. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍first ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍two ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍X_train ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍quantitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍after ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍qualitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns.\n\nfrom matplotlib.patches import Patch\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, bbox_to_anchor=(1, 1))\n      \n      plt.tight_layout()\n\n\n\nModel Choice\nI used combinations to experiment with all pairs of quantitative features and combined them with all categorical attributes like Sex, Clutch Completion, and Island. The goal was to identify the optimal feature combinations that would lead to the highest accuracy when trained on a Random Forest model.\nOnce the model was trained on each set of features, I assessed its accuracy on the test set. When I reached a perfect accuracy score of 100%, I plotted decision regions of the training data model and the test data model by calling the previous function to visualize the model’s decision boundaries. I wanted to ensure that I understood how well the classifier was generalizing and how different feature combinations might affect the model’s ability to classify penguins correctly. Additionally, I included confusion matrices to evaluate the classification results in more detail and to confirm that the model wasn’t simply overfitting.\nI only print the decision region plots and confusion matrix of the first model that achieves max accuracy. I continue to count how many other combinations I can make that achieves max accuracy. This count is usually been between 3 and 5 and may vary due to the nature of the way random forest classifiers build the model.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nall_qual_cols = [\"Sex\", \"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nplotted = False\ncount = 1\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols =  list(pair) + qual_cols\n\n    # train model and find best accuracy score  \n    # columns for the model that has the best score. \n    rf = RandomForestClassifier()\n    rf.fit(X_train[cols], y_train)\n    score  = rf.score(X_test[cols], y_test) * 100\n\n    if score == 100.0 and not plotted:\n      print(\"Training Set\")\n      plot_regions(rf, X_train[cols], y_train)\n      plt.show()\n      \n      print(\"Test Set\")\n      plot_regions(rf, X_test[cols], y_test)\n      plt.show()\n\n      y_pred = rf.predict(X_test[cols])\n      C = confusion_matrix(y_test, y_pred)\n      print(C)\n\n      plotted = True\n    \n    elif score == 100.0:\n      # print(cols)\n      count += 1\n    \nprint(\"\\nThe total number of combinations for which 100% accuracy was achieved\", count)\n\nTraining Set\n\n\n\n\n\n\n\n\n\nTest Set\n\n\n\n\n\n\n\n\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\nThe total number of combinations for which 100% accuracy was achieved 3\n\n\n\n\nDecision Region Plots\nThe first decision region graph for the Random Forest classifier displays the training data points, while the second shows the test data. It’s clear that the decision boundaries for male and female individuals across different species vary. For example, the color-coded regions for female Gentoo penguins are limited to a body mass of 4500g and a culmen length of 41mm, while the male Gentoo penguins’ regions extend up to a body mass of 6000g and a culmen length of 45mm, as indicated by the shaded blue areas. Similarly, the male Adelie penguins are confined to the upper right of the graph, while female Adelie penguins occupy the top half, as indicated by the shaded red regions. Female Chinstrap penguins show a larger range in culmen length, with a body mass limit of 4000g, while male Chinstrap penguins span a smaller range in culmen length but extend to a body mass of up to 4900g, as indicated by the shaded green regions.\nWhen analyzing the decision boundary on the test data, it becomes evident how well the Random Forest classifier generalizes to unseen data. The decision regions for each species and sex remain consistent with the training data. Most of the test data points fit within their respective color-coded species regions, indicating that the model has learned the underlying patterns effectively.\n\n\nConfusion Matrix\nThe confusion matrix clearly demonstrates the model’s 100% accuracy rate. By comparing the true labels with the labels predicted by the Random Forest classifier, it’s evident that the model made no false predictions. Every prediction aligns perfectly with the actual class, indicating that the model has successfully classified all test instances (31 Adelie, 11 Chinstrap, 26 Gentoo) without error.\n\n\nDiscussion\nIn this analysis, I learned several key insights about both the dataset and machine learning model performance. Firstly, the data required careful preprocessing, such as encoding categorical variables and handling missing data, to ensure effective model training. Through the creation of summary statistics and visualization of relationships between different features, I observed notable patterns in the data, such as the strong separation between penguin species based on physical traits like flipper length and culmen length. The Random Forest classifier, when trained on optimal feature combinations, achieved an impressive 100% accuracy, demonstrating its effectiveness in classifying penguins based on the available data.\nOne of the most fascinating parts of this process was visualizing decision regions for both the training and test datasets. It does change slightly just beacuse of the randomization during the process. The decision boundaries revealed how well the classifier learned to differentiate between species and sexes, with clear regions for each group, even for the test data that the model had not seen before."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Perceptron.py file.\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data, plot_perceptron_data, draw_line, plot_loss_curve, train_perceptron"
  },
  {
    "objectID": "posts/perceptron/index.html#linearly-separable-data",
    "href": "posts/perceptron/index.html#linearly-separable-data",
    "title": "Perceptron",
    "section": "Linearly ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍separable data",
    "text": "Linearly ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍separable data\nThe below code (taken from our class notes) generates a 2D dataset with 300 data points, where each point is labeled as either 0 or 1, using the perceptron_data function. The we plot these data points to get a visual representation, where we onbserve that it is in fact linearly seperable.\n\nfrom matplotlib import pyplot as plt\nimport torch\nplt.style.use('seaborn-v0_8-whitegrid')\n\nX, y = perceptron_data(n_points = 300, noise = 0.2) \n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nIn the context of linear separability, a dataset is considered linearly separable if there exists a hyperplane (defined by a weight vector) that can perfectly separate the two classes. From the following code output graph , we see that there is in fact a clear linear separation between the two classes, showing that a hyperplane could be drawn that separates the data points of one class from those of the other without any misclassifications. The two distinct clusters of data points (represented by brown circles and blue squares) are well separated in the feature space, and a straight line (hyperplane) could be used to separate the classes perfectly. This indicates that the dataset is linearly separable, and the perceptron algorithm should be able to find a weight vector that achieves perfect classification.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\nw_0 = torch.Tensor([1, -1, 0])\nw_1 = torch.Tensor([1,  1, -1]) \n\ndraw_line(w_0, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = r\"$w^{(0)}$\")\ndraw_line(w_1, 0, 1, ax, color = \"black\", label = r\"$w^{(1)}$\")\n\nl = ax.legend(ncol = 2)\n\n\n\n\n\n\n\n\nWe then begin the iterative training process of a perceptron, where the model’s weights are updated based on misclassified data points. Initially, a perceptron and its optimizer are set up, and the loss is calculated. In each iteration of the training loop, a random data point is selected, and if it’s misclassified (i.e., the loss is greater than 0), the perceptron’s weights are updated using the optimizer. The old and new decision boundaries are plotted, with misclassified points highlighted, and the loss is tracked over time. The training continues until the perceptron correctly classifies all points, minimizing the loss to zero. The visualization provides an understandable picture of how the perceptron progressively adjusts its decision boundary(the black solid line) to separate the two classes until we are able to achieve a loss of 0.\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nn = X.shape[0] \n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i)\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y)\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe above visualizations include two decision boundaries per plot: a solid line representing the current decision boundary, and a dashed line indicating either a previous boundary or an alternative. Over time, the perceptron algorithm progresses, with each plot reflecting different stages of the learning process. In the top-left plot, with a low loss value (0.020), the boundary is almost optimal, showing minimal misclassification. The top-middle plot (loss = 0.437) displays a poorly oriented boundary with high error, while the top-right plot (loss = 0.100) demonstrates an improved boundary with moderate error. In the bottom-left plot (loss = 0.000), the perceptron has converged to a perfect classification with no misclassifications, signaling that the optimal decision boundary has been found. The visualizations also highlight misclassified points: circles in the beige cluster and squares in the teal cluster, which triggered weight updates during training. The empty plots in the bottom row suggest no further updates were necessary after convergence.\n\nplot_loss_curve(loss_vec)\n\n\n\n\n\n\n\n\nThe loss curve for the perceptron training shows the algorithm’s learning process by plotting the perceptron’s loss function against iteration updates. Initially, at iteration 0, the loss is around 0.02, suggesting the starting weights perform reasonably well. However, after the first update (iteration 1), the loss increases sharply to approximately 0.43, indicating that the weight adjustments momentarily worsened the model’s performance. In iteration 2, the loss decreases to about 0.1 as the perceptron begins to converge towards a more optimal solution. By iteration 3, the loss reaches near zero, signifying that the perceptron has found weights that perfectly classify all data points. The x-axis, shows updates that occur when the perceptron encounters misclassified points. The curve’s non-monotonic shape—rising and then falling—reflects the typical nature of perceptron training, especially with specific learning rates. The final drop to zero loss indicates that the dataset is linearly separable, and the perceptron has successfully learned a decision boundary that classifies all data points correctly. This visualization highlights the iterative weight adjustments and shows how the perceptron quickly converges to an optimal solution, achieving perfect classification with just three updates, demonstrating efficient learning for this particular dataset."
  },
  {
    "objectID": "posts/perceptron/index.html#non-linear-data",
    "href": "posts/perceptron/index.html#non-linear-data",
    "title": "Perceptron",
    "section": "Non linear data",
    "text": "Non linear data\nWe generate another 2D dataset with 300 data points, where each point is labeled as either 0 or 1, using the perceptron_data function. This time we do it with a higher noise value of 0.8 to create a non-lineraly seperatable data set. The we plot these data points to get a visual representation, where we onbserve that the circle and squares merge into each other making it harder to create distinct visual boundaries.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data(n_points = 300, noise = 0.8) \nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\nw_0 = torch.Tensor([1, -1, 0])\nw_1 = torch.Tensor([1,  1, -1]) \n\ndraw_line(w_0, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = r\"$w^{(0)}$\")\ndraw_line(w_1, 0, 1, ax, color = \"black\", label = r\"$w^{(1)}$\")\n\nl = ax.legend(ncol = 2)\n\n\n\n\n\n\n\n\nFrom the following code output graph, we observe that the two clusters of data points (represented by brown circles and blue squares) are not cleanly separated by a straight line. The data points of one class still overlap with those of the other, and a single hyperplane cannot be drawn to separate the classes without misclassifications. We observe that the black solid line is still unable to separate the two types of points. This leads us to guess that we may not be able to achieve a perfect loss of 0.\nIn the case of a non-linearly separable dataset, the iterative training process of the perceptron follows a similar approach, but the outcome differs as the model struggles to perfectly classify the data. Initially, the perceptron and its optimizer are set up, and all the loss values are added to the loss_vec array so we can plot these updates.\n\n# initialize another perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = train_perceptron(X, y, p, opt)\n\nplot_loss_curve(loss_vec)\n\n\n\n\n\n\n\n\n\nThe graph displays the loss function of a perceptron algorithm applied to a non-linearly separable dataset with over approximately 250 iteration updates, highlighting an osciallating pattern. The loss fluctuates significantly, oscillating between roughly 0.20 and 0.50, with regular spikes reaching around 0.50 and valleys dipping to 0.20-0.25. This oscillation persists throughout the entire training period, indicating a lack of convergence. Unlike linearly separable data, the perceptron never achieves zero loss, and there is no stable downward trend. The explanation for this behavior is that for non-linearly separable datasets, no hyperplane can perfectly separate the classes, causing the perceptron to continually update weights and misclassify data points. Without a stopping criterion, this process would theoretically continue indefinitely. Since we have bounded it to 1000 siterations, we can see that even 250 update weren’t enough. This visualization demonstrates why standard perceptrons are unsuitable for non-linearly separable problems, necessitating our use of external stopping criteria (maximum iterations)."
  },
  {
    "objectID": "posts/perceptron/index.html#multidimensional",
    "href": "posts/perceptron/index.html#multidimensional",
    "title": "Perceptron",
    "section": "Multidimensional",
    "text": "Multidimensional\nSince we are unable to plot multidimentional data, we only create this data set by increasing the number of dimensions to 35 on a non linear data set(noise = 0.9). We then call the training function which append each loss update to our array which we then use to plot our loss curve.\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims=35) \n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = train_perceptron(X, y, p, opt)\n\nplot_loss_curve(loss_vec)\n\n\n\n\n\n\n\n\nI substited different values for the noise when create the data set for this perceptron. What I realized is that adding more dimensions doesnt necessarily change the behaviour of the model. For low noises, despite adding more dimentions(6), the loss value was able to get down to 0. When I increased the noise but kept the same number of dimensions(6), the loss value kept zigzagiing up and down similar to the solutuon to the prvious question. However, at insanely high dimension count/(&gt; 25), I did notice that loss value was able to reach zero even though the noise was also high (0.7 - 0.9) making the data set seem like it was linearly seperateable.\nThe image displays a loss curve illustrating the learning progression of a perceptron model over approximately 40 iterations with multidimensional input data. The curve begins at a moderate loss value of about 0.23, followed by a significant spike at iteration 3, where the loss jumps to around 0.38, likely due to a major weight adjustment as the perceptron encounters challenging data points. After this initial spike, the loss shows a consistent downward trend with smaller fluctuations, decreasing from 0.15-0.17 at iteration 10 to below 0.05 by iteration 30, with small spikes at iterations 10, 20, and 32 indicating temporary setbacks during model adjustments. As the iterations progress, the loss steadily approaches zero, reaching near-zero by iteration 40, signaling that the perceptron has successfully learned to classify the multidimensional data. The flattening of the curve toward the end suggests convergence, indicating that the perceptron has found an optimal hyperplane to separate the data classes in the multidimensional space. The overall decreasing trend confirms the perceptron’s success in minimizing classification errors, with the final near-zero loss suggesting that the data is likely linearly separable in the input space."
  },
  {
    "objectID": "posts/logistic_regression/index.html",
    "href": "posts/logistic_regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "logistic.py file.\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/logistic_regression/index.html#experimental-data",
    "href": "posts/logistic_regression/index.html#experimental-data",
    "title": "Implementing Logistic Regression",
    "section": "Experimental Data",
    "text": "Experimental Data\nCode ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍classification ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problem. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍You ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍can ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍control ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍points ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adjusting ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍n_points, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍features ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adjusting ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍p_dims, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍difficulty ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍classification ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problem ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adjusting ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍noise ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(higher ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍noise ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍harder ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problem).\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y"
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "This blog presents a fairness audit of income prediction using U.S. Census data via the folktables library and logistic regression. Focusing on gender and racial disparities, we explore how predictive performance and outcome distributions vary across demographic groups in California. Through descriptive statistics, visualizations, and model auditing including disaggregated accuracy, predictive values, and error rates, we uncover systemic gaps, particularly the underrepresentation of women and racial minorities among high earners. These findings highlight both model bias and broader socioeconomic inequities reflected in the data.\nNote: Some values may differ slightly as metrics vary every time model is created.\nAccessing data using folktables:\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem\nimport numpy as np\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)"
  },
  {
    "objectID": "posts/auditing-bias/index.html#figure-1",
    "href": "posts/auditing-bias/index.html#figure-1",
    "title": "Auditing Bias",
    "section": "Figure 1",
    "text": "Figure 1\nRace 1, the largest racial group in the dataset ( “White”), dominates in both income categories, with the highest number of individuals in each bracket. Within this group, males outnumber females in the higher income bracket (&gt;$50K), while females outnumber males in the lower income bracket (≤$50K). Race 2,representing “Black” or “African American,” is the second largest group and shows a similar trend: females are more numerous in the lower income bracket, while males outnumber females among those earning above $50K. Smaller racial groups (such as those labeled 6, 7, 8, and 9) have much lower representation and tend to have a more balanced gender distribution, with a slight male predominance in higher income brackets. Across nearly all racial groups, males are more likely to be in the higher income bracket, while females are more prevalent in the lower bracket. Overall, a visible gender gap exists in higher income brackets, with males more represented among higher earners. Race 1’s dominance in both high and low income brackets may reflect population distribution or socioeconomic disparities, while smaller racial groups have minimal representation in both categories.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_values = df[[\"label\", \"RAC1P\", \"group\"]].copy()\ndf_values[\"label\"] = df_values[\"label\"].apply(lambda x:  \"&gt; $50K\" if x == 1 else  \"&lt;= $50K\")\ndf_values[\"Sex\"] = df_values[\"group\"].apply(lambda x:  \"Male\" if x == 1 else  \"Female\")\n\nfig1 = sns.catplot(data = df_values, \n                x = \"RAC1P\", \n                hue = \"Sex\",\n                col=\"label\",\n                kind=\"count\",\n                palette=\"deep\")\n\nfig1.set_axis_labels(\"Race\", \"Count of indvidual within label earnings\")\nplt.show()\n\n\n\n\n\n\n\n\nFocusing on the column with label &lt; 50k, there is a clear trend indicating that, across all racial groups, a higher number of females earn less than $50K compared to males. In the income category above $50K, White males(race 1) are significantly overrepresented relative to females, suggesting a notable gender disparity. Among Black or African American individuals, there were slighly more females that males earning above $50K. For Asian and Other racial groups, males also tend to earn more, though the disparity is less pronounced. Earnings appear to be relatively balanced between genders among individuals identifying with two or more races(race 9)."
  },
  {
    "objectID": "posts/auditing-bias/index.html#figure-2",
    "href": "posts/auditing-bias/index.html#figure-2",
    "title": "Auditing Bias",
    "section": "Figure 2",
    "text": "Figure 2\nThe code begins by extracting and duplicating key columns (AGEP, PINCP, SEX, and RAC1P) from the original dataset for analysis. The SEX column is mapped to descriptive gender labels (“Male” or “Female”), while the RAC1P column is recoded to more interpretable race categories (e.g., “White”, “Asian”, “Black or African American”). Using Seaborn’s FacetGrid, the code creates a series of scatter plots showing the relationship between age (AGEP) and income (PINCP), with each subplot representing a different racial group. Data points are colored and styled by gender to facilitate comparison. Axis labels, legends, and titles are added for clarity, and the entire visualization is titled to reflect the focus on age-income patterns across race and gender.\n\ndf_fig_2 = acs_data[[\"AGEP\", \"PINCP\", \"SEX\", \"RAC1P\"]].copy()\n\ndf_fig_2[\"Sex\"] = df_fig_2[\"SEX\"].map({1: \"Male\", 2: \"Female\"})\nrace_map = {\n    1: \"White\",\n    2: \"Black or African American\",\n    3: \"American Indian\",\n    4: \"Alaska Native\",\n    5: \"American Indian/Alaska Native NOS\",\n    6: \"Asian\",\n    7: \"NH/Other Pacific Islander\",\n    8: \"Other\",\n    9: \"Two or More\"\n}\ndf_fig_2[\"Race\"] = df_fig_2[\"RAC1P\"].map(race_map)\n\ng = sns.FacetGrid(df_fig_2, col=\"Race\", col_wrap=3)\ng.map_dataframe(sns.scatterplot, x=\"AGEP\", y=\"PINCP\", hue=\"Sex\", style=\"Sex\", alpha=0.5)\n\ng.set_axis_labels(\"Age\", \"Income (PINCP)\")\ng.add_legend(title=\"Gender\")\ng.set_titles(col_template=\"Race: {col_name}\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Scatter Plot of Age vs Income Faceted by Race and Colored by Gender\")\nplt.show()\n\n\n\n\n\n\n\n\nThe income distribution across all races is highly skewed, with the majority of individuals earning lower incomes and a smaller group earning significantly higher amounts. There is no clear linear relationship between age and income, as the income distribution is widely dispersed across ages, although higher incomes tend to be more common in middle-aged groups.\nLooking at race-specific trends, White individuals exhibit the broadest spread in both age and income, with a noticeable concentration of high-income outliers above $600,000. Males and females are represented across the income range, though high-income outliers are more prominent in this group. For Asians, while there are some high-income outliers, most incomes remain below $400,000, and the gender distribution among higher earners is relatively balanced. Black or African American individuals tend to have incomes below $200,000, with fewer high-income outliers compared to Whites and Asians. The gender distribution within this group is fairly even. Those identifying as Two or More Races or Other also exhibit lower income levels, with most incomes below $200,000 and a few outliers, though their sample sizes are smaller and the distribution is less pronounced. American Indian, Alaska Native, and NH/Other Pacific Islander groups have the lowest income ranges and smallest sample sizes, with very few individuals earning above $100,000.\nIn terms of gender patterns, both males and females are represented across the income spectrum in most racial categories, although there does not appear to be a consistent gender gap in income within each group. However, in both the White and Asian groups, higher earners are more often male. As for age trends, higher incomes are predominantly seen among individuals aged 30 to 60, with fewer high-income earners at younger or older ages. There is no strong evidence suggesting that income consistently increases with age, but income tends to be more variable among middle-aged individuals. The sample size also plays a role, with White individuals making up the largest group, followed by Asians and Black or African Americans. Smaller sample sizes in other racial categories make it difficult to observe clear trends."
  },
  {
    "objectID": "posts/auditing-bias/index.html#overall-measures",
    "href": "posts/auditing-bias/index.html#overall-measures",
    "title": "Auditing Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\nThe following code evaluates the performance of a logistic regression model by calculating key metrics such as test accuracy, confusion matrix, and derived metrics including Predictive Positive Value (PPV), False Positive Rate (FPR), and False Negative Rate (FNR). It first computes the accuracy by comparing the true labels (y_test) with the predicted labels (y_pred). The confusion matrix is then used to derive four values: true negatives, false positives, false negatives, and true positives, which are used to calculate PPV (the proportion of positive predictions that are correct), FPR (the rate at which negative samples are misclassified as positive), and FNR (the rate at which positive samples are misclassified as negative). The output shows an accuracy of 85.4%, with a PPV of 72.6%, FPR of 7.7%, and FNR of 36.1%, highlighting the model’s strengths in predicting negatives correctly, but also revealing areas for improvement in minimizing false negatives.\n\ny_pred = LR_search.predict(X_test)\nprint(\"Overall Metrics:\")\n\ndef evaluate_metrics(y_true, y_pred, group_name=None):\n    if group_name == 1:\n        print(f\"\\nMetrics for Males:\")\n    elif group_name == 2:\n        print(f\"\\nMetrics for Females:\")\n\n    test_accuracy = accuracy_score(y_true, y_pred)\n    print(f\"Test Accuracy: {test_accuracy:.3f}\")\n    \n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n    print(\"Confusion Matrix:\")\n    print(cm)\n\n    tn, fp, fn, tp = cm.ravel()\n\n    PPV = tp / (tp + fp)  \n    FPR = fp / (fp + tn)  \n    FNR = fn / (tp + fn) \n\n    print(f\"Predictive Value (PPV): {PPV:.3f}\")\n    print(f\"False Positive Rate (FPR): {FPR:.3f}\")\n    print(f\"False Negative Rate (FNR): {FNR:.3f}\")\n\nevaluate_metrics(y_test, y_pred)\n\nOverall Metrics:\nTest Accuracy: 0.854\nConfusion Matrix:\n[[52928  4437]\n [ 6651 11748]]\nPredictive Value (PPV): 0.726\nFalse Positive Rate (FPR): 0.077\nFalse Negative Rate (FNR): 0.361"
  },
  {
    "objectID": "posts/auditing-bias/index.html#by-group-measures",
    "href": "posts/auditing-bias/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe then evaluate the performance of this logistic regression model for both males and females separately by calculating key metrics again such as test accuracy, confusion matrix, and derived metrics like Predictive Positive Value (PPV), False Positive Rate (FPR), and False Negative Rate (FNR). It first computes the accuracy for each gender by comparing the true labels (y_test) with the predicted labels (y_pred). The confusion matrix for each group provides the counts of true negatives, false positives, false negatives, and true positives, which are then used to calculate PPV (the accuracy of positive predictions), FPR (the rate of misclassifying negatives as positives), and FNR (the rate of misclassifying positives as negatives). For males, the accuracy is 83.6%, with a PPV of 77.2%, FPR of 7.8%, and FNR of 36.7%. For females, the accuracy is higher at 87.0%, with a PPV of 66.8%, FPR of 7.6%, and FNR of 35.3%. These results show that the model performs better for females, with higher accuracy and slightly better FPR, but both genders show similar FNR values, indicating opportunities for improving the model’s handling of false negatives.\n\n# There can be cases where testing set only containy one group\ngroups = np.unique(group_test)\n\nfor group in groups:\n    y_label_group = y_test[group_test == group]\n    y_pred_group = y_pred[group_test == group]\n\n    evaluate_metrics(y_label_group, y_pred_group, group)\n\n\nMetrics for Males:\nTest Accuracy: 0.836\nConfusion Matrix:\n[[24281  2067]\n [ 4050  6985]]\nPredictive Value (PPV): 0.772\nFalse Positive Rate (FPR): 0.078\nFalse Negative Rate (FNR): 0.367\n\nMetrics for Females:\nTest Accuracy: 0.870\nConfusion Matrix:\n[[28647  2370]\n [ 2601  4763]]\nPredictive Value (PPV): 0.668\nFalse Positive Rate (FPR): 0.076\nFalse Negative Rate (FNR): 0.353"
  },
  {
    "objectID": "posts/auditing-bias/index.html#bias-measures",
    "href": "posts/auditing-bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nThis following code generates calibration plots to assess how well the predicted probabilities from a model align with the actual outcomes, both overall and for specific subgroups. It first calculates the calibration curve for the entire dataset, comparing the predicted probabilities against the true positive rates, and visualizes it alongside a perfectly calibrated line.\n\nfrom sklearn.calibration import calibration_curve\n\nprob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n\nplt.plot(prob_pred, prob_true, marker='o', label='Model')\nplt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of True Positives')\nplt.title('Calibration Plot Overall (Reliability Diagram)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe function plot_calibration_by_group extends this analysis by plotting calibration curves for each subgroup within the dataset, allowing for a detailed comparison of how well the model is calibrated across different groups.\n\ndef plot_calibration_by_group(y_test, y_pred, group_test, groups):\n    for group in groups:\n        y_test_group = y_test[group_test == group]\n        y_pred_group = y_pred[group_test == group]\n        \n        prob_true, prob_pred = calibration_curve(y_test_group, y_pred_group, n_bins=10)\n        plt.plot(prob_pred, prob_true, marker='o', label=f'Group {group}')\n    \n    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\n    \n    plt.xlabel('Mean Predicted Probability')\n    plt.ylabel('Fraction of True Positives')\n    plt.title('Calibration Plot by Group (Reliability Diagram)')\n    plt.legend()\n    plt.show()\n\ngroups = np.unique(group_test)  \n\n# Call the function to plot calibration curves for each group\nplot_calibration_by_group(y_test, y_pred, group_test, groups)"
  },
  {
    "objectID": "posts/quant-limits/index.html",
    "href": "posts/quant-limits/index.html",
    "title": "Limits of the Quantitative Approach",
    "section": "",
    "text": "Quantitative methods are widely used to assess bias and discrimination in machine learning models, yet their effectiveness remains a topic of debate. While these methods—such as statistical fairness criteria, audits, and hypothesis testing—provide objective ways to detect disparities, they also have limitations. Narayanan (2022) critiques these approaches, arguing that they often reinforce systemic bias rather than mitigate it. This essay engages with Narayanan’s claims by analyzing both the benefits and drawbacks of quantitative fairness assessments. Using examples from facial recognition technology and Facebook’s housing ad delivery algorithms, it explores the trade-offs between mathematical fairness notions and broader ethical concerns. Drawing from Barocas, Hardt, and Narayanan (2023) and D’Ignazio and Klein (2023), this analysis highlights how fairness metrics alone cannot resolve deep-seated inequalities. Instead, a combined approach—integrating statistical methods with qualitative and policy-driven interventions—is essential for achieving meaningful algorithmic fairness."
  },
  {
    "objectID": "posts/overfitting-DD/index.html",
    "href": "posts/overfitting-DD/index.html",
    "title": "Overfitting and Double Descent",
    "section": "",
    "text": "Overfitting, Overparameterization, and Double-Descent.py file.\n\n%load_ext autoreload\n%autoreload 2\nfrom Overparameterization_dd import LinearModel, MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\n\nAbstract\nThis post explores the behavior of overparameterized linear regression models by investigating the double descent phenomenon using random feature transformations. We examine how training and test errors evolve as the number of features increases. By applying a nonlinear random feature mapping (with a square activation) and computing optimal weights that interpolate the training data, we visualize the resulting error trends. Our results confirm the presence of the double descent curve: training error decreases monotonically with more features, while test error initially decreases, peaks around the interpolation threshold, and then decreases again. These findings illustrate how increasing model capacity beyond the interpolation point can paradoxically improve generalization.\nStarter code from assignment:\n\nimport torch\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\n\nPart 0\nIn Equation 1, (X^T )^-1 can only exist is (X^T) invertible. And this can only happen when the number of data points (n) is greater than or equal to the number of features (p). So if we try to calculate this result, we would end up with an underfined value for the inverse calculation which means the solution for the closed form calculation does not exist.\n\n\nPart A\nI implemented the MyLinearRegression class with the requested functions: predict, loss.\nThe OverParameterizedLinearRegressionOptimizer class computes the optimal weights for the linear regression model using the given Moore-Penrose pseudoinverse calculation technique.\n\n\nPart B\nFitting ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍model ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍some ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍1-d ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.grid()\n\n\n\n\n\n\n\n\nUsing ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍RandomFeatures ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍class ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍set ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍features ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍then ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍feed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍MyLinearRegression ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍model. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍We then ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍model ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍predictions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍alongside ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data: ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\n\nphi = RandomFeatures(n_features=8)\nphi.fit(X)\nX_random = phi.transform(X)\n\nmodel = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(model)\n\nopt.fit(X_random, y)\n\ny_pred = model.predict(X_random)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, y_pred, color='blue', label='Predictions')\nplt.title('Overparameterized Linear Regression')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPart C: Double Descent In Image Corruption Detection\nMost of the following code was provided.\nNow ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍apply ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍model ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍more ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍complicated ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍set. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Here’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍our ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍scenario: ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍following ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍greyscale ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍beautiful ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍flower:\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nHowever, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍this ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍has ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍some ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍variable ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍amount ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍corruption ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍represented ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍contiguous ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍blocks ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍grey ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pixels, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍implemented ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍following ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adds ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍random ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍randomly-sized ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍grey ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patches ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍returns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patches ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍added.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nOur ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍task ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍predict ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍corruptions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍just ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍from ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍itself. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍To ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍do ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍this, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍set ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍corrupted ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍images.\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nNow ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍reshape ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍(laying ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍its ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pixels ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍out ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍it ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍one ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍long ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍row) ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍split ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍test ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nThe following code tries to understand the behavior of an overparameterized linear regression model using random feature transformations and visualizes the double descent phenomenon in both training and test error. We loop over a range of features up to 200. For each number of features, we apply a RandomFeatures transformation (with a square activation) to the training and testing data. We then compute the optimal weights that interpolate the training data. After training the model, the code calculates the mean squared error (MSE) on both the training and testing sets using the loss function and stores these values.\nFinally, we plot the training and test MSE side by side on a log-scaled y-axis. A vertical line is drawn at the interpolation threshold to highlight this transition point. These visualizations help illustrate how the training error decreases steadily with more features, while the test error first decreases, spikes around the interpolation threshold, and then decreases again—demonstrating the classic double descent curve observed in overparameterized models.\n\nn_range = list(range(1, 201))\ntrain_errors = []\ntest_errors = []\n\nfor n_features in n_range:\n    phi = RandomFeatures(n_features=n_features, activation=square)\n    phi.fit(X_train)\n    X_train_random = phi.transform(X_train)\n    X_test_random = phi.transform(X_test)\n\n    model = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(model)\n    opt.fit(X_train_random, y_train)\n\n    train_errors.append(model.loss(X_train_random, y_train))\n    test_errors.append(model.loss(X_test_random, y_test))\n\ninterpolation_threshold = X_train.size(0)\n\n# Plotting the two graphs side by side with 1 row, 2 columns\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\naxs[0].scatter(range(200), train_errors, color='grey')\naxs[0].set_yscale('log')\naxs[0].axvline(x = interpolation_threshold, color = 'black', ymax=0.05)\naxs[0].set_xlabel(\"Number of features\")\naxs[0].set_ylabel(\"Mean squared error (Training)\")\n\naxs[1].scatter(range(200), test_errors, color='red')\naxs[1].set_yscale('log')\naxs[1].axvline(x = interpolation_threshold, color = 'black', ymax=0.05)\naxs[1].set_xlabel(\"Number of features\")\naxs[1].set_ylabel(\"Mean squared error (Testing)\")\n\nplt.show()\n\n\n\n\n\n\n\n\nAs expected we observe the double descent phenomenon.\nThe ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍features ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍at ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which I ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍achieved ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍best ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍testing ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍error was around 195 with the lowest mean squared error value. This optimal number is above the interpolation threshold.\nTraditionally, by increasing model complexity through increasing number of features, we expect to cause overfitting and thus higher test error. However, this phenonmenon of double descent we have just observed shows that after the peak in test error at the interpolation threshold, further increasing the number of features (with risk of over-parameterization) we actually observe a second decrease in test error, sometimes achieving even lower error than at the classical “sweet spot” of the bias-variance curve.\n\n\nDiscussion\nThrough this analysis, we observed a clear instance of the double descent phenomenon in a linear regression setting enhanced by random nonlinear features. Contrary to the classical bias-variance tradeoff intuition, test error did not simply worsen with increasing model complexity. Instead, it peaked near the interpolation threshold and then improved, with the best performance occurring at a feature count well beyond this point. This challenges the traditional assumption that overparameterization necessarily leads to worse generalization. Completing this post deepened my understanding of modern generalization behavior in machine learning and showed that we can sometimes improve model performance by pushing past conventional limits. It helped me realize that, with the right feature transformations, we can go beyond the apparent ceiling imposed by overfitting and build models that perform surprisingly well—even though it is highly overparameterized."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog by Kusuma Kumar",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nUnderstanding logistic regression with vanilla and momentum-based gradient descent.\n\n\n\n\n\nMay 17, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting and Double Descent\n\n\n\n\n\nExplorating double descent in overparameterized linear regression using random feature transformations.\n\n\n\n\n\nMay 16, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nA fairness audit of income prediction using U.S. Census data with folktables and logistic regression, with emphasis on gender and race group disparities.\n\n\n\n\n\nMay 7, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nAnalyze credit risk using loan data and logistic regression to predict defaults based on borrower demographics. It also applies profit-driven threshold optimization to enhance automated lending decisions.\n\n\n\n\n\nMay 5, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\n\n\nThis blog explores the perceptron algorithm, its application to linearly and non linearly separable data, and its behavior in higher-dimensional spaces. It also demonstrates the model’s learning process and limitations.\n\n\n\n\n\nMar 19, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach\n\n\n\n\n\nExploring the limitations of a purely quantitative approach to understanding algorithmic bias and discrimination.\n\n\n\n\n\nMar 19, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins with a Random Forest Classifier and visualizing the decision regions generated by the model.\n\n\n\n\n\nFeb 21, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\nNo matching items"
  }
]