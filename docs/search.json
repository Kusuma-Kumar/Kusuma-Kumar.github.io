[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nThis blog post explores the process of building a machine learning classifier using the Palmer Penguins dataset. The analysis focuses on the identification of key features and the construction of a Random Forest model to classify penguin species based on various measurements. Through data preparation, feature engineering, and model training, we assess how well different combinations of features influence the classifier’s performance. The results reveal that a perfect accuracy of 100% can be achieved, with decision region plots visualizing the classifier’s ability to distinguish between species and sexes. The findings highlight the significant impact of feature selection and the effectiveness of Random Forest classifiers in this scenario.\nAcess the training data and test data\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\n\nData Preparation\nYou ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍will ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍need to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍prepare ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ ‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍qualitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Categorical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍feature ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍like ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Sex ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Island ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍should ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍converted ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍so-called ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍“one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍encoded” ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍0-1 ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍using ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pd.get_dummies ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍label ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍column ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Species ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍should ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍be ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍coded ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍differently, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍using ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍LabelEncoder. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍following ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍handles ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍this ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍work ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍you.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_test, y_test = prepare_data(test)\n\n\n\nSummary table\nThe following code processes the “Species” column of a train DataFrame by splitting any compound species names and extracting only the first word (e.g., “Adelie” from “Adelie Penguin”). It then creates a new DataFrame df that includes only the relevant columns: “Body Mass (g)”, “Island”, and “Species”. Next, it groups the data by both “Species” and “Island” to compute three statistics for the “Body Mass (g)” column: the mean, median, and count of observations. This aggregation provides a summary table that reveals the average body mass (mean), the middle value (median), and the number of penguin observations (count) for each species on each island, allowing for a clear comparison of body mass distributions across different species and locations.\nBased on the summary statistics provided, it’s clear that only Adelie penguins are found on all three islands, while Chinstrap and Gentoo penguins are restricted to only specific islands - Dream and Biscoe respectively. Adelie species data reveals that living on different islands has a negligible impact on body mass as mean and median body mass values for Adelie penguins are almost identical across the three islands. In contrast, Chinstrap penguins on Dream Island have a mean body mass of around 3743g, and Gentoo penguins on Biscoe Island have a much higher mean of 5039g, reflecting the species’ differing size characteristics.\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ndf = train[[\"Body Mass (g)\", \"Island\", \"Species\"]]\n\ndf.groupby(['Species' ,'Island' ])['Body Mass (g)'].agg(\n    Mean='mean',\n    Median='median',\n    Count='count'\n)\n\n\n\n\n\n\n\n\n\nMean\nMedian\nCount\n\n\nSpecies\nIsland\n\n\n\n\n\n\n\nAdelie\nBiscoe\n3711.363636\n3750.0\n33\n\n\nDream\n3728.888889\n3700.0\n45\n\n\nTorgersen\n3712.804878\n3700.0\n41\n\n\nChinstrap\nDream\n3743.421053\n3700.0\n57\n\n\nGentoo\nBiscoe\n5039.948454\n5000.0\n97\n\n\n\n\n\n\n\n\n\nFigure 1\nThe code below visualizes the relationship between Flipper Length and Culmen Length across the different penguin species. The scatterplot clearly reveals three distinct clusters corresponding to the three species: Adelie, Gentoo, and Chinstrap. The Adelie species primarily lies in the lower-left corner of the plot, while Gentoo penguins are clustered in the upper-left, and Chinstrap penguins are positioned towards the lower-right. Despite the clear separation, there are a few data points that overlap between species. Notably, there is some mixing of Adelie and Chinstrap penguins in the middle of the plot, which suggests potential similarities in their flipper and culmen lengths, making them harder to distinguish in certain cases.\n\nimport seaborn as sns\n\ndf = train[[\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Species\"]]\nfigure1 = sns.scatterplot(df, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\n\n\n\nFigure 2\nI wanted to explore the relationship between Sex, Culmen Length, and Delta 15 N to examine potential differences between male and female penguins within the Chinstrap species. Specifically, I was curious to see if there were any notable patterns or trends in these two measurements that could vary by sex. By plotting Culmen Length (mm) against Delta 15 N, we can get a sense of how these two variables correlate and whether there are any distinguishing features between males and females of the Chinstrap species.\nFrom the scatter plot, we observe that both males and females within the Chinstrap species show a similar spread across Delta 15 N, but with the females’ values slightly lower. The notable difference, however, is that females seem to have shorter Culmen Lengths compared to males, although there is slight overlap around 50–51 mm. There is also one noticeable outlier: a female Chinstrap with the highest Culmen Length of 58 mm.\n\ndf = train[[\"Species\", \"Sex\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nfiltered_df = df[df['Species'] == \"Chinstrap\"]\nfigure2 = sns.scatterplot(filtered_df, x = \"Culmen Length (mm)\", y = \"Delta 15 N (o/oo)\", hue = \"Sex\", style = \"Sex\")\n\n\n\n\n\n\n\n\n\n\nUsing Findings from Figure to Create Model\nThe scatterplots showing the relationship between flipper length and culmen length revealed distinct clusters for each penguin species, which highlighted the importance of these features for classification. I used this information to carefully select and pair quantitative features that showed strong separation between species, such as flipper and culmen lengths, to train the Random Forest classifier. Additionally, the scatterplot of Culmen Length vs. Delta 15 N for Chinstrap penguins provided further insights into the potential influence of sex on species classification, prompting me to include the ‘Sex’ feature in the model. The decision region plots helped me visualize how different feature combinations influenced the classifier’s ability to distinguish between species. This visualization reinforced the importance of feature selection and showed me how decision boundaries could be altered with different combinations of features.\n\n\nPlotting Decision Regions\nNow ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍going ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍panel ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍decision ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍classifier. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍You ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍can ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍use ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot_regions ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍defined ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍below ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regions. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍This ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍function ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assumes ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍your ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍first ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍two ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍X_train ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍quantitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍after ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍are ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍qualitative ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍columns.\n\nfrom matplotlib.patches import Patch\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, bbox_to_anchor=(1, 1))\n      \n      plt.tight_layout()\n\n\n\nModel Choice\nI used combinations to experiment with all pairs of quantitative features and combined them with all categorical attributes like Sex, Clutch Completion, and Island. The goal was to identify the optimal feature combinations that would lead to the highest accuracy when trained on a Random Forest model.\nOnce the model was trained on each set of features, I assessed its accuracy on the test set. When I reached a perfect accuracy score of 100%, I plotted decision regions of the training data model and the test data model by calling the previous function to visualize the model’s decision boundaries. I wanted to ensure that I understood how well the classifier was generalizing and how different feature combinations might affect the model’s ability to classify penguins correctly. Additionally, I included confusion matrices to evaluate the classification results in more detail and to confirm that the model wasn’t simply overfitting.\nI only print the decision region plots and confusion matrix of the first model that achieves max accuracy. I continue to count how many other combinations I can make that achieves max accuracy. This count is usually been between 3 and 5 and may vary due to the nature of the way random forest classifiers build the model.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nall_qual_cols = [\"Sex\", \"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\nplotted = False\ncount = 1\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols =  list(pair) + qual_cols\n\n    # train model and find best accuracy score  \n    # columns for the model that has the best score. \n    rf = RandomForestClassifier()\n    rf.fit(X_train[cols], y_train)\n    score  = rf.score(X_test[cols], y_test) * 100\n\n    if score == 100.0 and not plotted:\n      print(\"Training Set\")\n      plot_regions(rf, X_train[cols], y_train)\n      plt.show()\n      \n      print(\"Test Set\")\n      plot_regions(rf, X_test[cols], y_test)\n      plt.show()\n\n      y_pred = rf.predict(X_test[cols])\n      C = confusion_matrix(y_test, y_pred)\n      print(C)\n\n      plotted = True\n    \n    elif score == 100.0:\n      # print(cols)\n      count += 1\n    \nprint(\"\\nThe total number of combinations for which 100% accuracy was achieved\", count)\n\nTraining Set\n\n\n\n\n\n\n\n\n\nTest Set\n\n\n\n\n\n\n\n\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\nThe total number of combinations for which 100% accuracy was achieved 3\n\n\n\n\nDecision Region Plots\nThe first decision region graph for the Random Forest classifier displays the training data points, while the second shows the test data. It’s clear that the decision boundaries for male and female individuals across different species vary. For example, the color-coded regions for female Gentoo penguins are limited to a body mass of 4500g and a culmen length of 41mm, while the male Gentoo penguins’ regions extend up to a body mass of 6000g and a culmen length of 45mm, as indicated by the shaded blue areas. Similarly, the male Adelie penguins are confined to the upper right of the graph, while female Adelie penguins occupy the top half, as indicated by the shaded red regions. Female Chinstrap penguins show a larger range in culmen length, with a body mass limit of 4000g, while male Chinstrap penguins span a smaller range in culmen length but extend to a body mass of up to 4900g, as indicated by the shaded green regions.\nWhen analyzing the decision boundary on the test data, it becomes evident how well the Random Forest classifier generalizes to unseen data. The decision regions for each species and sex remain consistent with the training data. Most of the test data points fit within their respective color-coded species regions, indicating that the model has learned the underlying patterns effectively.\n\n\nConfusion Matrix\nThe confusion matrix clearly demonstrates the model’s 100% accuracy rate. By comparing the true labels with the labels predicted by the Random Forest classifier, it’s evident that the model made no false predictions. Every prediction aligns perfectly with the actual class, indicating that the model has successfully classified all test instances (31 Adelie, 11 Chinstrap, 26 Gentoo) without error.\n\n\nDiscussion\nIn this analysis, I learned several key insights about both the dataset and machine learning model performance. Firstly, the data required careful preprocessing, such as encoding categorical variables and handling missing data, to ensure effective model training. Through the creation of summary statistics and visualization of relationships between different features, I observed notable patterns in the data, such as the strong separation between penguin species based on physical traits like flipper length and culmen length. The Random Forest classifier, when trained on optimal feature combinations, achieved an impressive 100% accuracy, demonstrating its effectiveness in classifying penguins based on the available data.\nOne of the most fascinating parts of this process was visualizing decision regions for both the training and test datasets. It does change slightly just beacuse of the randomization during the process. The decision boundaries revealed how well the classifier learned to differentiate between species and sexes, with clear regions for each group, even for the test data that the model had not seen before."
  },
  {
    "objectID": "posts/quant-limits/index.html",
    "href": "posts/quant-limits/index.html",
    "title": "Limits of the Quantitative Approach",
    "section": "",
    "text": "Quantitative methods are widely used to assess bias and discrimination in machine learning models, yet their effectiveness remains a topic of debate. While these methods—such as statistical fairness criteria, audits, and hypothesis testing—provide objective ways to detect disparities, they also have limitations. Narayanan (2022) critiques these approaches, arguing that they often reinforce systemic bias rather than mitigate it. This essay engages with Narayanan’s claims by analyzing both the benefits and drawbacks of quantitative fairness assessments. Using examples from facial recognition technology and Facebook’s housing ad delivery algorithms, it explores the trade-offs between mathematical fairness notions and broader ethical concerns. Drawing from Barocas, Hardt, and Narayanan (2023) and D’Ignazio and Klein (2023), this analysis highlights how fairness metrics alone cannot resolve deep-seated inequalities. Instead, a combined approach—integrating statistical methods with qualitative and policy-driven interventions—is essential for achieving meaningful algorithmic fairness."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog by Kusuma Kumar",
    "section": "",
    "text": "Limits of the Quantitative Approach\n\n\n\n\n\nExploring the limitations of a purely quantitative approach to understanding algorithmic bias and discrimination.\n\n\n\n\n\nMar 19, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins with a Random Forest Classifier and visualizing the decision regions generated by the model.\n\n\n\n\n\nFeb 21, 2025\n\n\nKusuma Kumar\n\n\n\n\n\n\nNo matching items"
  }
]